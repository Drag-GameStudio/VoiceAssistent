## Executive Navigation Tree
* üìÑ Core Engine
  * [LLM Engine Component](#llm-engine-component)
  * [Component Responsibility](#component-responsibility)
  * [Specific Component Responsibility](#specific-component-responsibility)
  * [Key Components](#key-components)
  * [Code Structure](#code-structure)
  * [System Architecture](#system-architecture)
  * [Data Flow](#data-flow)
  * [Logic Flow](#logic-flow)
  * [Technical Logic Flow](#technical-logic-flow)
  * [Technical Logic Flow 2](#technical-logic-flow-2)
* üìÇ Data Contract
  * [Data Contract](#data-contract)
  * [Data Contract 2](#data-contract-2)
  * [Data Contract for Services](#data-contract-for-services)
  * [Data Contract for Auto Create Service](#data-contract-for-auto-create-service)
* üìä Service Execution
  * [Visible Interactions](#visible-interactions)
  * [Visible Interactions 2](#visible-interactions-2)
  * [Service Execution Workflow](#service-execution-workflow)
  * [Auto Create Service Description](#auto-create-service-description)
  * [Auto Create Service Workflow](#auto-create-service-workflow)
  * [Example Use Case for Auto Create Service](#example-use-case-for-auto-create-service)
  * [Auto Search Service](#auto-search-service)
* üìù Documentation
  * [Functional Role](#functional-role)
  * [Error Handling](#error-handling)
  * [Warnings](#warnings)
  * [Critical Logic Assumptions](#critical-logic-assumptions)
  * [Project Knowledge Base](#project-knowledge-base)
  * [Documentation Requirements](#documentation-requirements)
* üó£Ô∏è Voice Assistant
  * [Voice Assistant Documentation](#voice-assistant-documentation)
  * [Voice Listening Algorithms](#voice-listening-algorithms)
  * [Vosk VLA Class](#vosk-vla-class)
  * [Cloud VLA Class](#cloud-vla-class)
  * [Cloud VLA PyAudio Class](#cloud-vla-pyaudio-class)
  * [Voice Listen Algorithms Implementation](#voice-listen-algorithms-implementation)
  * [VLA Text Class](#vlatext-class)
  * [VL Manager Class](#vlmanager-class)

 

<a name="llm-engine-component"></a>
## LLM Engine Component
The LLM Engine Component is a part of the VoiceAssistent system, responsible for handling user requests and generating responses using a large language model (LLM). 
<a name="component-responsibility"></a>
## Component Responsibility
The component is responsible for **managing the Voice Assistant's** functionality. It handles the voice activation, voice listening, and voice acting components. The `run_multi_va_and_task` function manages the execution of voice activation and tasks concurrently. 
<a name="specific-component-responsibility"></a>
## Specific Component Responsibility
The `runner.py` component is responsible for **managing the execution of voice activation and tasks concurrently**. It uses the `run_multi_va_and_task` function to create a new thread for the task and a new process for the voice activation. 
<a name="key-components"></a>
## Key Components
The key components of the system are:
* **Voice Activation (VA)**: Responsible for activating the voice assistant.
* **Voice Listening (VL)**: Responsible for listening to user input.
* **Voice Acting (VA)**: Responsible for generating responses to user input.
* **Language Processing**: Responsible for processing user input and generating responses. 
<a name="code-structure"></a>
## Code Structure
The code is organized into several files and directories:
* **engine/engines/promts.py**: Contains the `general_prompt_create` function and the `DONATIK_ID` prompt.
* **manage.py**: The main entry point of the application, responsible for initializing the voice assistant.
* **process_control/init_control.py**: Provides a class for initializing and quitting processing. 
<a name="system-architecture"></a>
## System Architecture
The VoiceAssistent project employs a modular architecture, allowing for the easy integration of new services. The architecture consists of the following key components:
- **Service Manager**: Acts as the central dispatcher, routing user requests to the appropriate services.
- **Services**: Implement the `BaseService` class, providing a `global_handle` method that defines how the service processes input arguments. 
<a name="data-flow"></a>
## Data Flow
The data flow of the system is as follows:
| Entity | Type | Role | Notes |
| --- | --- | --- | --- |
| User Input | Text | Input | User input to the voice assistant. |
| Voice Activation | Boolean | Output | Indicates whether the voice assistant is activated. |
| Voice Listening | Audio | Input | Audio input from the user. |
| Voice Acting | Text | Output | Response generated by the voice assistant. |
| Language Processing | Text | Input/Output | Processes user input and generates responses. | 
<a name="logic-flow"></a>
## Technical Logic Flow
The technical logic flow of the component is as follows:
1. The `run_multi_va_and_task` function creates a new thread for the task and a new process for the voice activation.
2. The `VAManager.listen_micro` function is executed in the voice activation process, which listens to the microphone for voice input.
3. The `handler_func` function is executed in the task thread, which handles the voice input and generates a response.
4. The `create_handler` function creates a handler function for the voice input, which calls the `run_multi_va_and_task` function. 
<a name="technical-logic-flow"></a> Technical Logic Flow
The technical logic flow of the Auto Search Service can be broken down into the following steps:

1. **Initialization**: The Auto Search Service is initialized as an instance of the `AutoSearch` class.
2. **Search Query Handling**: The `fast_search` method is called with a search query and optional parameters (e.g., `max_results`).
3. **Search Result Retrieval**: The `fast_search` method uses the `DDGS` library to retrieve search results based on the provided query.
4. **Result Processing**: The retrieved search results are processed and returned as a dictionary containing the results and a status indicator.
5. **Webpage Content Retrieval**: The `get_page_content` method is called with a URL to retrieve the webpage content.
6. **Content Parsing**: The retrieved webpage content is parsed using `BeautifulSoup` to extract relevant text.

### 
<a name="technical-logic-flow-2"></a>
## Technical Logic Flow
The technical logic flow of the `runner.py` component is as follows:
1. The `run_multi_va_and_task` function creates a new thread for the task and a new process for the voice activation.
2. The `VAManager.listen_micro` function is executed in the voice activation process, which listens to the microphone for voice input.
3. The `handler_func` function is executed in the task thread, which handles the voice input and generates a response.
4. The `create_handler` function creates a handler function for the voice input, which calls the `run_multi_va_and_task` function. 
<a name="data-contract"></a> Data Contract
The Auto Search Service expects the following inputs and produces the corresponding outputs:

| Entity | Type | Role | Notes |
| --- | --- | --- | --- |
| `query` | string | Input | Search query |
| `max_results` | integer | Input | Optional parameter to limit search results |
| `link` | string | Input | Webpage URL for content retrieval |
| `results` | list | Output | Search results |
| `text` | string | Output | Retrieved webpage content |

### 
<a name="data-contract-2"></a>
## Data Contract
The `runner.py` component expects the following inputs:
| Entity | Type | Role | Notes |
| --- | --- | --- | --- |
| Request | String | Input | Voice input from the user. |
| VA Manager | VAManager | Input | Voice activation manager. |
| VActing Manager | VActingManager | Input | Voice acting manager. |
| Engine Manager | EngineManager | Input | Engine manager. |

The `runner.py` component produces the following outputs:
| Entity | Type | Role | Notes |
| --- | --- | --- | --- |
| Response | String | Output | Voice response generated by the voice assistant. |
| Error | Exception | Output | Error message generated by the voice assistant. |

### Project Parameters Update

The provided code snippets and project structure have updated the project parameters for the VoiceAssistent project as follows:

- **global_idea**: The project seems to revolve around creating a modular, extensible voice assistant system that can run various services.
- **target_audience**: The target audience appears to be developers and potentially end-users who will interact with the voice assistant.
- **tech_stack**: The technology stack includes Python, with specific use of modules like `importlib` for dynamic module importing, and regular expressions for text processing.
- **system_components**: The system comprises multiple components:
  - **Service Manager**: Responsible for routing user requests to specific software modules.
  - **Initialization Module**: Provides a base prompt for the AI dispatcher, listing available modules and their descriptions.
  - **Reader Module**: Parses commands to execute services with their respective arguments.
  - **Runner Module**: Executes services based on parsed commands.
  - **Base Service**: A base class for all services, with a `global_handle` method that services must implement.

### Documentation in Line with the Updated Parameters 
<a name="data-contract-for-services"></a>
## Data Contract for Services
Services expect and return data in the following format:
| Entity | Type | Role | Notes |
| --- | --- | --- | --- |
| Service Name | String | Input | Name of the service to be executed. |
| Arguments | Dictionary | Input | Key-value pairs of arguments required by the service. |
| Response | String | Output | Result of the service execution. |
| Error | Exception | Output | Error message if service execution fails. |

### Example Use Case
For a user request to check the weather, the Service Manager would invoke the `Weather` service with the required arguments (e.g., location). The `Weather` service would then execute its `global_handle` method to fetch and return the current weather for the specified location.

```python
# Example invocation of the Weather service
RUNMODULE weather.Weather|location="NewYork"
```

This modular design enables developers to add new services by simply implementing the `BaseService` class and registering their service in the `INSTALLED_APPS` list, without modifying the existing system architecture. 
<a name="data-contract-for-auto-create-service"></a>
## Data Contract for Auto Create Service
The Auto Create service expects and returns data in the following format:
| Entity | Type | Role | Notes |
| --- | --- | --- | --- |
| `what_need` | String | Input | Description of the task to be accomplished. |
| `result` | Dictionary | Output | Result of the code execution, containing `global_status`, `message`, and any additional data. |
| `global_status` | Boolean | Output | Indicator of whether the task was successful. |
| `message` | String | Output | Success message or error description. |
| `data` | Variable | Output | Any additional data required by the task. | 
<a name="visible-interactions"></a> Visible Interactions
The Auto Search Service interacts with the following components:

* `BaseService`: The Auto Search Service inherits from the BaseService class, which provides a basic structure for handling requests and responses.
* `DDGS`: The Auto Search Service uses the DDGS library to perform searches and retrieve results.
* `requests` and `BeautifulSoup`: The Auto Search Service utilizes these libraries to fetch and parse webpage content.

### 
<a name="visible-interactions-2"></a>
## Visible Interactions
The `runner.py` component interacts with other parts of the system through the following functions:
* `VAManager.listen_micro`: Listens to the microphone for voice input.
* `handler_func`: Handles the voice input and generates a response.
* `create_handler`: Creates a handler function for the voice input. 
<a name="service-execution-workflow"></a>
## Service Execution Workflow
1. **User Request Analysis**: The Service Manager analyzes the user's request to determine which service to invoke.
2. **Validation**: The Service Manager checks if all required arguments for the selected service are present in the user's input.
3. **Service Invocation**: If all arguments are present, the Service Manager invokes the service using the `RUNMODULE` command, passing the service name and required arguments.
4. **Service Execution**: The invoked service executes its `global_handle` method with the provided arguments and returns the result. 
<a name="auto-create-service-description"></a>
## Auto Create Service Description
The Auto Create service is a part of the VoiceAssistent project, leveraging the capabilities of the `BaseService` class and the Groq API for generating Python code based on user input. This service is designed to generate and execute Python code for tasks that do not have existing modules or require custom implementation. 
<a name="auto-create-service-workflow"></a>
## Auto Create Service Workflow
1. **User Request Receipt**: The service receives a user request in the form of a string, specifying the task that needs to be accomplished.
2. **Code Generation**: The service utilizes the Groq API to generate Python code based on the user's request. This involves creating a prompt that includes the task description, expected output format, and any necessary libraries.
3. **Code Preparation and Execution**: The generated code is then prepared for execution by checking for required libraries and installing them if necessary. The code is executed in a controlled environment, and the output is captured and processed.
4. **Error Handling and Retry**: If the code execution fails, the service attempts to retry the execution up to a specified number of times, adjusting the code based on previous errors.
5. **Result Processing and Return**: The final output of the code execution is processed and returned as a dictionary, containing a status indicator, a message, and any additional data required by the task. 
<a name="example-use-case-for-auto-create-service"></a>
## Example Use Case for Auto Create Service
For a user request to "generate a Python function to calculate the area of a rectangle," the Auto Create service would:
1. Generate Python code using the Groq API.
2. Prepare and execute the generated code.
3. Return the result as a dictionary, containing the calculated area and any additional data.

```python
# Example invocation of the Auto Create service
what_need = "generate a Python function to calculate the area of a rectangle"
result = Auto().handle(what_need)
print(result)
```

This service enables users to request custom Python code generation and execution for a wide range of tasks, making it a versatile tool for the VoiceAssistent project.

## 
<a name="auto-search-service"></a> Auto Search Service
The Auto Search Service is a module within the VoiceAssistent project, responsible for searching and retrieving information from the internet.

### 
<a name="functional-role"></a> Functional Role
The Auto Search Service plays a crucial role in the VoiceAssistent project by providing the ability to search for information online. It can be used to retrieve relevant data based on a given query or by providing a direct link to a webpage.

### 
<a name="error-handling"></a> Error Handling
The Auto Search Service catches exceptions during the search and content retrieval processes, returning an error message with a status indicator. 
<a name="warnings"></a>
## Warnings
The system uses a third-party API (Groq) which may have usage limits or require additional configuration. The system also stores conversation history in the database, which may have security implications.

iohttp==3.7.3
aiohttp-client-session==0.7.0
aiosignal==1.1.2
alembic==1.4.3
async-timeout==3.0.1
attrs==21.4.0
blosc==1.10.2
certifi==2021.10.8
click==8.0.3
Cython==0.29.24
dill==0.3.4
dogpile.cache==1.2.0
fastjsonschema==2.15.1
Flask-Cors==3.0.10
Flask-SQLAlchemy==2.5.1
Flask==2.0.2
greenlet==1.1.2
idna==3.3
itsdangerous==2.0.1
Jinja2==3.0.3
MarkupSafe==2.0.1
numpy==1.21.4
pathos==0.2.8
path==15.0.1
pyparsing==3.0.7
python-engineio==4.2.0
python-socketio==5.1.0
pytz==2021.3
psutil==5.8.0
pydantic==1.8.2
python-dateutil==2.8.2
PyYAML==6.0
requests==2.27.1
scipy==1.7.3
setproctitle==1.1.10
six==1.16.0
SQLAlchemy==1.4.29
SQLAlchemy-Utils==0.37.8
textworld==1.1.0
threadpoolctl==3.0.0
toml==0.10.2
typing-extensions==4.0.1
Werkzeug==2.0.3
wheel==0.37.1
zope.event==1.5.0
zope.interface==5.4.0
zstandard==0.15.2



Project Parameters: 
global_idea: AI-powered Voice Assistant
target_audience: Tech-Savvy and Non-Tech-Savvy Users
tech_stack: Python 3, Pathos, Psutil, Aiohttp, Flask, SQLALchemy
key_features: Voice Activation, Voice Listening, Language Processing, Voice Acting
system_description: AI-powered Voice Assistant that listens to voice commands and generates voice responses
functional_requirements: 
* The system should be able to understand voice commands and respond accordingly.
* The system should be able to activate/deactivate the voice assistant based on user input.
* The system should be able to process multiple voice commands simultaneously.
* The system should be able to provide a smooth voice response.
* The system should be able to handle exceptions and errors gracefully. 
<a name="critical-logic-assumptions"></a> Critical Logic Assumptions
> The Auto Search Service assumes that the `DDGS` library is properly configured and functional.
> The Auto Search Service assumes that the provided webpage URL is valid and accessible.

### 
<a name="project-knowledge-base"></a>
## Project Knowledge Base
The project name is **VoiceAssistent**.

### Project Parameters
Based on the provided information, the project parameters are:
* **global_idea**: Create a voice assistant with a witty and concise personality.
* **target_audience**: Users who want a helpful and charming voice assistant.
* **tech_stack**: Python, Django, PyAudio, Groq API, and other libraries.
* **key_features**: Voice activation, voice listening, voice acting, and language processing. 
<a name="documentation-requirements"></a>
## Documentation Requirements
To create high-fidelity documentation for the provided code fragments, the following sections are required:
* **Specific Component Responsibility**: Describe the exact functional role of each fragment within the system.
* **Visible Interactions**: Explain how each piece communicates with other parts of the system.
* **Technical Logic Flow**: Provide a step-by-step breakdown of the classes, functions, and internal logic.
* **Data Contract**: Detail the flow of inputs, outputs, and side effects in table format. 
<a name="voice-assistant-documentation"></a>
## Voice Assistant Documentation
### Overview of Voice Assistant Components

The Voice Assistant system consists of several key components, including:

*   **PyAudioManager**: A singleton class responsible for managing PyAudio streams.
*   **PlayAudioManager**: A singleton class responsible for playing audio files.
*   **BaseVActingAlgorithm**: A base class for voice acting algorithms.
*   **EdgeVActingAlgorithm** and **GoogleVActingAlgorithm**: Concrete implementations of voice acting algorithms.

### PyAudioManager
#### Class Description

The `PyAudioManager` class is a singleton responsible for managing PyAudio streams. It provides methods for starting and stopping streams, as well as retrieving information about the current stream.

#### Methods

*   `start_stream(*args, **kwargs)`: Starts a new PyAudio stream with the provided arguments.
*   `stop_stream()`: Stops the current PyAudio stream.
*   `get_current_stream_info()`: Returns information about the current PyAudio stream, including format, channels, rate, and frames per buffer.
*   `get_index()`: Returns the index of the default input device.

#### Example Usage

```python
py_audio_manager = PyAudioManager()
stream = py_audio_manager.start_stream(format=pyaudio.paInt16, channels=1, rate=44100, input=True, frames_per_buffer=1024)
```

### PlayAudioManager
#### Class Description

The `PlayAudioManager` class is a singleton responsible for playing audio files. It provides methods for playing audio files synchronously and asynchronously.

#### Methods

*   `get_file_path(sound_name)`: Returns the file path of the audio file to be played.
*   `play_sound_process(file_path)`: Plays the audio file at the specified file path.
*   `play_sound(sound_name, is_thread=True, with_daemon=False)`: Plays the audio file with the specified name, optionally in a separate thread.

#### Example Usage

```python
play_audio_manager = PlayAudioManager()
play_audio_manager.play_sound("example.mp3")
```

### BaseVActingAlgorithm
#### Class Description

The `BaseVActingAlgorithm` class is a base class for voice acting algorithms. It provides a basic structure for implementing voice acting algorithms.

#### Methods

*   `acting(request)`: This method must be implemented by subclasses to perform the voice acting logic.

### EdgeVActingAlgorithm
#### Class Description

The `EdgeVActingAlgorithm` class is a concrete implementation of the `BaseVActingAlgorithm` class. It uses the Edge TTS library to synthesize text into speech.

#### Methods

*   `__init__(voice="ru-RU-SvetlanaNeural")`: Initializes the algorithm with the specified voice.
*   `acting(request)`: Synthesizes the provided text into speech using the Edge TTS library and plays the resulting audio file.

#### Example Usage

```python
edge_algorithm = EdgeVActingAlgorithm(voice="ru-RU-SvetlanaNeural")
edge_algorithm.acting("Hello, world!")
```

### GoogleVActingAlgorithm
#### Class Description

The `GoogleVActingAlgorithm` class is a concrete implementation of the `BaseVActingAlgorithm` class. It uses the Google Text-to-Speech library to synthesize text into speech.

#### Methods

*   `__init__(lang="ru")`: Initializes the algorithm with the specified language.
*   `acting(request)`: Synthesizes the provided text into speech using the Google Text-to-Speech library and plays the resulting audio file.

#### Example Usage

```python
google_algorithm = GoogleVActingAlgorithm(lang="ru")
google_algorithm.acting("Hello, world!")
```

### Data Contract
#### Inputs

| Entity | Type | Role | Notes |
| --- | --- | --- | --- |
| `request` | `str` | Input text to be synthesized into speech |  |
| `voice` | `str` | Voice to be used for synthesis | Optional, defaults to "ru-RU-SvetlanaNeural" |
| `lang` | `str` | Language to be used for synthesis | Optional, defaults to "ru" |

#### Outputs

| Entity | Type | Role | Notes |
| --- | --- | --- | --- |
| `audio_file` | `str` | Path to the synthesized audio file |  |

#### Side Effects

*   The synthesized audio file is played using the `PlayAudioManager` class.

### Technical Logic Flow

1.  The `EdgeVActingAlgorithm` or `GoogleVActingAlgorithm` class is instantiated with the desired voice or language.
2.  The `acting` method is called on the instantiated algorithm, providing the input text to be synthesized into speech.
3.  The algorithm uses the Edge TTS or Google Text-to-Speech library to synthesize the input text into speech, generating an audio file.
4.  The synthesized audio file is played using the `PlayAudioManager` class.

### Critical Logic Assumptions

> The Edge TTS and Google Text-to-Speech libraries are assumed to be installed and configured correctly.
> The `PlayAudioManager` class is assumed to be functioning correctly and able to play audio files.
> The input text is assumed to be in the correct format and encoding for the selected voice or language.

### Project Context
The project is centered around a voice assistant system, with various components working together to provide voice activation and voice acting capabilities.

### Project Parameters
The following parameters are defined for the project:
* **global_idea**: Voice assistant system
* **target_audience**: Users who want to interact with their devices using voice commands
* **tech_stack**: Python, PyAudio, pvporcupine, openwakeword, and other relevant libraries

### Voice Activation Algorithms
The project includes several voice activation algorithms, including:
* **BaseVAAlgorithm**: A base class for voice activation algorithms
* **PVAlgorithm**: An algorithm using the pvporcupine library for voice activation
* **VAText**: An algorithm that does not use any library for voice activation

### Voice Acting Algorithms
The project also includes several voice acting algorithms, including:
* **BaseVActingAlgorithm**: A base class for voice acting algorithms
* **VActingText**: An algorithm that simply prints the input text

### System Components
The project consists of several system components, including:
* **VAManager**: A manager for voice activation algorithms
* **VActingManager**: A manager for voice acting algorithms
* **PyAudioManager**: A manager for PyAudio
* **PlayAudioManager**: A manager for playing audio files

### Technical Logic Flow
The technical logic flow of the system is as follows:
1. The user speaks a voice command.
2. The **VAManager** uses a voice activation algorithm to detect the voice command.
3. If the voice command is detected, the **VAManager** triggers an action.
4. The **VActingManager** uses a voice acting algorithm to synthesize the text into speech.
5. The synthesized speech is played using the **PlayAudioManager**.

### Data Contract
The data contract for the system is as follows:
#### Inputs
| Entity | Type | Role | Notes |
| --- | --- | --- | --- |
| `audio_frame` | `bytes` | Input audio frame |  |
| `text` | `str` | Input text to be synthesized into speech |  |

#### Outputs
| Entity | Type | Role | Notes |
| --- | --- | --- | --- |
| `result` | `bool` | Result of voice activation |  |
| `audio_file` | `str` | Path to the synthesized audio file |  |

#### Side Effects
* The synthesized audio file is played using the **PlayAudioManager**.
* The **VAManager** triggers an action when a voice command is detected.

### Critical Logic Assumptions
> The voice activation algorithms are assumed to be working correctly and detecting voice commands.
> The voice acting algorithms are assumed to be working correctly and synthesizing text into speech.
> The **PyAudioManager** and **PlayAudioManager** are assumed to be working correctly and playing audio files. 
<a name="voice-listening-algorithms"></a>
## Voice Listening Algorithms
The provided code snippet appears to be part of a voice-controlled system, specifically designed for listening to and processing voice commands. The system utilizes various voice listening algorithms to recognize and interpret user voice inputs.

### Class Overview
There are three main classes in the provided code snippet:
* `VoskVLA`
* `CloudVLA`
* `CloudVLAPyAudio`

Each class seems to implement a different approach to voice listening and recognition.

### VoskVLA Class 
<a name="vosk-vla-class"></a>
#### VoskVLA Class
The `VoskVLA` class utilizes the Vosk speech recognition system to recognize voice commands. It has the following methods:
* `__init__`: Initializes the Vosk model and recognizer.
* `configurate`: Configures the PyAudio stream for voice input.
* `listen_micro`: Listens to the microphone and recognizes voice commands using the Vosk recognizer.

### CloudVLA Class 
<a name="cloud-vla-class"></a>
#### CloudVLA Class
The `CloudVLA` class uses the Google Cloud Speech-to-Text API to recognize voice commands. It has the following methods:
* `__init__`: Initializes the speech recognition object and sets the language and timeout.
* `prep`: Prepares the microphone for voice input.
* `listen_micro`: Listens to the microphone and recognizes voice commands using the Google Cloud Speech-to-Text API.

### CloudVLAPyAudio Class 
<a name="cloud-vla-pyaudio-class"></a>
#### CloudVLAPyAudio Class
The `CloudVLAPyAudio` class uses PyAudio to listen to voice commands and recognize them using the Google Cloud Speech-to-Text API. It has the following methods:
* `__init__`: Initializes the speech recognition object and sets the language and timeout.
* `get_rms`: Calculates the root mean square (RMS) of the audio signal.
* `recognize_text`: Recognizes voice commands using the Google Cloud Speech-to-Text API.
* `listen_micro`: Listens to the microphone and recognizes voice commands.

### Data Contract
The following table outlines the input, output, and parameters for each class:

| Entity | Type | Role | Notes |
| --- | --- | --- | --- |
| `lang` | string | input | Language code (e.g., "ru", "en") |
| `timeout` | float | input | Timeout for voice recognition (in seconds) |
| `audio_data` | bytes | input | Raw audio data from the microphone |
| `text` | string | output | Recognized text from the voice command |
| `error` | string | output | Error message if voice recognition fails |

### Technical Logic Flow
The technical logic flow for each class is as follows:
1. Initialize the speech recognition object and set the language and timeout.
2. Prepare the microphone for voice input.
3. Listen to the microphone and collect audio data.
4. Recognize the voice command using the speech recognition API.
5. Send the recognized text to the next stage for processing.

Note that the specific implementation details may vary depending on the class and the speech recognition API used. 

> **Important:** The voice acting algorithms are assumed to be working correctly and synthesizing text into speech.
> **Important:** The **PyAudioManager** and **PlayAudioManager** are assumed to be working correctly and playing audio files. 
<a name="voice-listen-algorithms-implementation"></a>
## Voice Listen Algorithms Implementation

The provided code snippet implements voice listening algorithms using various approaches. The main classes involved are `VLAText` and `VLManager`.

### VLAText Class 
<a name="vlatext-class"></a>
#### VLAText Class
The `VLAText` class inherits from `VLABase` and provides a simple text-based interface for voice listening. It has the following methods:
* `__init__`: Initializes the `VLAText` object.
* `listen_micro`: Listens to the user input and calls the `handler_func` with the input text.

### VLManager Class 
<a name="vlmanager-class"></a>
#### VLManager Class
The `VLManager` class manages the voice listening process using a provided `VLABase` object. It has the following methods:
* `__init__`: Initializes the `VLManager` object with a `VLABase` object.
* `listen_micro`: Configures the PyAudio stream using the `PyAudioManager` singleton, listens to the microphone using the provided `VLABase` object, and starts the middleware action to stop listening.

### Visible Interactions
The `VLManager` class interacts with the `VLABase` object to listen to the microphone and recognize voice commands. It also uses the `PyAudioManager` singleton to configure the PyAudio stream and the `middleware_object` to start the action to stop listening.

### Technical Logic Flow
The technical logic flow for the `VLManager` class is as follows:
1. Initialize the `VLManager` object with a `VLABase` object.
2. Configure the PyAudio stream using the `PyAudioManager` singleton.
3. Listen to the microphone using the provided `VLABase` object.
4. Start the middleware action to stop listening.

### Data Contract
The following table outlines the input, output, and parameters for the `VLAText` and `VLManager` classes:

| Entity | Type | Role | Notes |
| --- | --- | --- | --- |
| `vla` | `VLABase` | input | Voice listening algorithm object |
| `text` | string | input | User input text |
| `request` | string | output | Recognized text from the voice command |
| `error` | string | output | Error message if voice recognition fails |

> **Important:** The `VLABase` object is assumed to be working correctly and providing the necessary functionality for voice listening.
> **Important:** The `PyAudioManager` and `middleware_object` are assumed to be working correctly and providing the necessary functionality for PyAudio stream configuration and middleware actions. 

Note that the specific implementation details may vary depending on the `VLABase` object used and the speech recognition API employed. 

### Class Relationship
The `VLAText` class inherits from `VLABase`, while the `VLManager` class uses a `VLABase` object to manage the voice listening process. The `VoskVLA` class is also a subclass of `VLABase` and can be used with the `VLManager` class. 

```python
# Example usage:
vla = VoskVLA(lambda request: print("lambda", request))
vl_manager = VLManager(vla)
vl_manager.listen_micro()
``` 
